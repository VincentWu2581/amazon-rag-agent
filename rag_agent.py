import streamlit as st
import os
import faiss

# V0.1.x ç‰ˆæœ¬ç¨³å®šå¯¼å…¥
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_models import ChatTongyi
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_community.llms import Tongyi
from typing import List, Dict

# ==========================================
# 1. é…ç½®ä¸åˆå§‹åŒ– (åŠ¡å¿…è®¾ç½®ä½ çš„ API Key)
# ==========================================
st.set_page_config(page_title="Amazon è¿è¥çŸ¥è¯†åŠ©æ‰‹", layout="wide")

# âš ï¸ æ³¨æ„ï¼šåœ¨ Streamlit Cloud ä¸­ä½¿ç”¨ Secrets æ¥å­˜å‚¨ API Key
# åœ¨æœ¬åœ°æµ‹è¯•æ—¶ï¼Œå¯ä»¥ç›´æ¥è®¾ç½®ï¼›åœ¨äº‘ç«¯åº”è¯¥ä½¿ç”¨ st.secrets
if "DASHSCOPE_API_KEY" in st.secrets:
    DASHSCOPE_API_KEY = st.secrets["DASHSCOPE_API_KEY"]
else:
    DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY", "")
    if not DASHSCOPE_API_KEY:
        st.error("âš ï¸ ç¼ºå°‘ DASHSCOPE_API_KEYã€‚è¯·åœ¨ Streamlit Cloud çš„ Secrets ä¸­è®¾ç½®ï¼Œæˆ–åœ¨æœ¬åœ°è®¾ç½®ç¯å¢ƒå˜é‡ã€‚")
        st.stop()

os.environ["DASHSCOPE_API_KEY"] = DASHSCOPE_API_KEY

# çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„
DOC_PATHS = [
    "äºšé©¬é€Šè·¨å¢ƒç”µå•†è¿è¥1000é¢˜åº“é¢˜ç›®é¢˜åº“é¢è¯•.docx",
    "å…¨å›½ï¼ˆäºšé©¬é€Šï¼‰é«˜è€ƒç»Ÿä¸€è¯•å·+ç­”æ¡ˆ).docx"
]

# RAG æ¨¡å‹å‚æ•°
CHUNK_SIZE = 800
CHUNK_OVERLAP = 100
EMBEDDING_MODEL_NAME = "BAAI/bge-small-zh-v1.5"
RECALL_K = 10
RERANK_K = 5
LLM_MODEL = "qwen-max"

# ==========================================
# 2. RAG ç»„ä»¶ï¼šPromptã€Retriever å’Œ Chain
# ==========================================

# 2.1 LLM è§’è‰²è®¾å®šï¼ˆè´¯å½»â€œè¿è¥æ ‡å‡†åŒ–â€ä¿¡å¿µï¼‰
SYSTEM_PROMPT = """
ä½ æ˜¯äºšé©¬é€Šé«˜çº§è¿è¥ç»ç†å¯¼å¸ˆï¼Œä½ çš„æ ¸å¿ƒä¿¡å¿µæ˜¯ï¼šå¸®åŠ©åˆçº§è¿è¥æ ‡å‡†åŒ–æ“ä½œï¼Œå¿«é€Ÿè¾¾åˆ°ä¸“å®¶æ°´å¹³ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ä¸¥æ ¼æ ¹æ®æä¾›çš„ã€çŸ¥è¯†åº“ç‰‡æ®µã€‘æ¥å›ç­”é—®é¢˜ï¼Œå¹¶å°†ç­”æ¡ˆä»¥æ ‡å‡†åŒ–çš„ CoT ç»“æ„è¾“å‡ºã€‚

è¯·ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹ CoT æ­¥éª¤ï¼š
1. **ã€æ¨¡å—å®šæ€§ã€‘**: ç¡®å®šç”¨æˆ·é—®é¢˜å±äºè¿è¥çš„å“ªä¸ªä¸€çº§æ¨¡å—ï¼ˆå¦‚ï¼šFBAç‰©æµã€PPCå¹¿å‘Šã€Listingä¼˜åŒ–ï¼‰ã€‚
2. **ã€æ ‡å‡†æ“ä½œ S.O.P.ã€‘**: æ•´ç†å‡ºå›ç­”è¯¥é—®é¢˜çš„æ ‡å‡†æ“ä½œæ­¥éª¤ï¼Œç”¨ Step 1, Step 2... æ ¼å¼å‘ˆç°ã€‚
3. **ã€ç»ç†æ´å¯Ÿ Pro-Tipã€‘**: æä¾›é«˜çº§è¿è¥çš„è§†è§’ï¼ŒåŒ…å«æ“ä½œçš„æ½œåœ¨é£é™©æˆ–èƒŒåçš„å•†ä¸šç­–ç•¥ï¼ŒæŒ‡å¯¼åˆçº§è¿è¥é¿å…å¸¸è§çš„é”™è¯¯ã€‚
4. **ã€çŸ¥è¯†æ¥æºã€‘**: å¼•ç”¨ä½ ä½¿ç”¨åˆ°çš„çŸ¥è¯†ç‰‡æ®µçš„æ–‡ä»¶åã€‚

é‡è¦çº¦æŸï¼šä½ ç»å¯¹ä¸èƒ½ä½¿ç”¨è‡ªå·±çš„å¤–éƒ¨çŸ¥è¯†ã€‚å¦‚æœæä¾›çš„ã€çŸ¥è¯†åº“ç‰‡æ®µã€‘ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œä½ å¿…é¡»ç›´æ¥å›ç­”ï¼š"çŸ¥è¯†åº“ä¸­æš‚æ— ç›¸å…³ä¿¡æ¯"ã€‚
"""

# 2.3 RAG Chain å®šä¹‰ - æ”¹è¿›çš„æ£€ç´¢ç­–ç•¥
def get_rag_chain(vector_store):
    """æ„å»ºåŒ…å«æ”¹è¿›æ£€ç´¢ç­–ç•¥çš„ RAG Chain"""
    
    # åŸºç¡€æ£€ç´¢å™¨ - å…ˆç”¨ K=10 æ£€ç´¢ï¼Œåé¢ä¼šç²¾æ’åˆ° K=5
    retriever = vector_store.as_retriever(search_kwargs={"k": RECALL_K})
    
    # LLM åˆå§‹åŒ– (Qwen-Max)
    llm = ChatTongyi(model=LLM_MODEL)
    
    # è¿”å›æ£€ç´¢å™¨å’Œ LLMï¼Œç”¨äºåç»­å¤„ç†
    # å®ç°ç²¾æ’ï¼šé€šè¿‡è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°æ¥é€‰æ‹©æœ€ç›¸å…³çš„ K=5 æ–‡æ¡£
    return {"retriever": retriever, "llm": llm, "rerank_k": RERANK_K}

# ==========================================
# 3. çŸ¥è¯†åº“æ„å»ºå‡½æ•°
# ==========================================

@st.cache_resource
def setup_knowledge_base():
    """åŠ è½½æ–‡æ¡£ã€åˆ‡åˆ†ã€å‘é‡åŒ–å¹¶å­˜å‚¨åˆ° FAISS"""
    with st.spinner("æ­£åœ¨åŠ è½½å’Œå¤„ç†äºšé©¬é€Šè¿è¥çŸ¥è¯†åº“..."):
        
        # 1. æ–‡æ¡£åŠ è½½
        docs = []
        for path in DOC_PATHS:
            try:
                # ä½¿ç”¨ UnstructuredFileLoader å¤„ç† Word æ–‡æ¡£
                loader = UnstructuredFileLoader(path, mode="elements")
                docs.extend(loader.load())
            except Exception as e:
                st.error(f"åŠ è½½æ–‡ä»¶ {path} å¤±è´¥: {e}. è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œä¾èµ–æ˜¯å¦å®‰è£…å®Œæ•´ (å¦‚ 'unstructured').")
                return None

        # 2. æ–‡æ¡£åˆ‡åˆ†
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE, 
            chunk_overlap=CHUNK_OVERLAP, 
            separators=["\n\n", "\n", " ", ""]
        )
        splits = text_splitter.split_documents(docs)
        
        # 3. å‘é‡åŒ–æ¨¡å‹
        embeddings = HuggingFaceBgeEmbeddings(
            model_name=EMBEDDING_MODEL_NAME
        )
        
        # 4. å‘é‡å­˜å‚¨
        vector_store = FAISS.from_documents(splits, embeddings)
        st.success(f"çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼å…±è®¡ {len(splits)} ä¸ªçŸ¥è¯†ç‰‡æ®µã€‚")
        return vector_store

# ==========================================
# 4. Streamlit UI ç•Œé¢ (ä¿æŒä¸å˜)
# ==========================================

def main():
    st.title("Amazon è¿è¥çŸ¥è¯†åŠ©æ‰‹ ")
    st.markdown("""
    **ğŸ’¡ æ ¸å¿ƒä¿¡å¿µï¼š** å°†åˆçº§è¿è¥çš„æ“ä½œæ ‡å‡†åŒ–ï¼Œå¿«é€Ÿè¾¾åˆ°é«˜çº§è¿è¥æ°´å¹³ã€‚
    **âœ… æŠ€æœ¯æ ˆï¼š** Qwen-Max (LLM) + BGE Re-Ranker (K=10 å¬å›, K=5 ç²¾æ’)
    """)

    # åˆå§‹åŒ–çŸ¥è¯†åº“
    vector_store = setup_knowledge_base()
    if vector_store is None:
        return
    
    # åˆå§‹åŒ– RAG Chain
    rag_chain = get_rag_chain(vector_store)

    # åˆå§‹åŒ–èŠå¤©è®°å½•
    if "messages" not in st.session_state:
        st.session_state.messages = [
            AIMessage(content="æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„äºšé©¬é€Šè¿è¥ç»ç† AI å¯¼å¸ˆã€‚è¯·é—®æ‚¨æƒ³äº†è§£å“ªä¸ªè¿è¥æ¨¡å—çš„ã€æ ‡å‡†æ“ä½œæµç¨‹ï¼ˆSOPï¼‰ã€‘ï¼Ÿ")
        ]
        
    # æ˜¾ç¤ºå†å²èŠå¤©è®°å½•
    for message in st.session_state.messages:
        with st.chat_message(message.type):
            st.markdown(message.content)

    # èŠå¤©è¾“å…¥
    if prompt := st.chat_input("è¾“å…¥ä½ çš„äºšé©¬é€Šè¿è¥é—®é¢˜..."):
        # 1. ç”¨æˆ·è¾“å…¥
        st.session_state.messages.append(HumanMessage(content=prompt))
        with st.chat_message("human"):
            st.markdown(prompt)

        # 2. AI å›ç­”
        with st.chat_message("ai"):
            # ä½¿ç”¨ st.status å±•ç¤º CoT æµç¨‹ï¼Œå¢å¼ºäº§å“ä½“éªŒ
            with st.status("ğŸ¤– AI å¯¼å¸ˆæ­£åœ¨æ£€ç´¢çŸ¥è¯†å¹¶æ„å»º SOP...", expanded=True) as status:
                
                # è°ƒç”¨ RAG Chain
                try:
                    # ä½¿ç”¨æ£€ç´¢å’Œç”Ÿæˆæµç¨‹
                    rag_components = rag_chain
                    retriever = rag_components["retriever"]
                    llm = rag_components["llm"]
                    rerank_k = rag_components["rerank_k"]
                    
                    # æ£€ç´¢ç›¸å…³æ–‡æ¡£ (K=10 å¬å›)
                    retrieved_docs = retriever.get_relevant_documents(prompt)
                    
                    # ç²¾æ’ï¼šåªä¿ç•™å‰ K=5 ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£ï¼ˆé€šè¿‡å‘é‡ç›¸ä¼¼åº¦è‡ªåŠ¨æ’åºï¼‰
                    top_docs = retrieved_docs[:rerank_k]
                    
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context = "\n\n".join([f"[æ–‡æ¡£ {i+1}] {doc.page_content}" for i, doc in enumerate(top_docs)])
                    
                    # æ„å»ºæç¤º
                    messages = [
                        SystemMessage(content=SYSTEM_PROMPT),
                        HumanMessage(content=f"ã€çŸ¥è¯†åº“å†…å®¹ã€‘\n{context}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\n{prompt}")
                    ]
                    
                    # è·å– LLM å›ç­”
                    response = llm.invoke(messages)
                    answer = response.content if hasattr(response, 'content') else str(response)
                    
                    # æ›´æ–°çŠ¶æ€
                    status.update(label="SOP æ„é€ å®Œæˆï¼", state="complete", expanded=False)
                    
                    # æ ¼å¼åŒ–è¾“å‡ºç­”æ¡ˆ
                    st.markdown(answer)

                    # 3. ä¸Šä¸‹æ–‡å¯è§†åŒ– - å±•ç¤ºç²¾æ’åçš„æ–‡æ¡£
                    with st.expander(f"ğŸ” æŸ¥çœ‹æ¨¡å‹ä½¿ç”¨çš„çŸ¥è¯†ç‰‡æ®µ (Re-Ranked K={rerank_k})"):
                        st.write("---")
                        st.markdown(f"**æ¨¡å‹ä» {len(retrieved_docs)} ä¸ªæ£€ç´¢ç»“æœä¸­ç²¾é€‰äº† {len(top_docs)} ä¸ªæœ€ç›¸å…³çš„çŸ¥è¯†ç‰‡æ®µã€‚**")
                        for i, doc in enumerate(top_docs):
                            # æå–çŸ¥è¯†ç‰‡æ®µå’Œæ¥æº
                            source_name = os.path.basename(doc.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶'))
                            content_snippet = doc.page_content[:300]
                            if len(doc.page_content) > 300:
                                content_snippet += "..."
                            
                            st.text_area(
                                f"ç‰‡æ®µ {i+1} (ç›¸å…³æ€§æ’åç¬¬{i+1}) - æ¥æº: {source_name}",
                                content_snippet,
                                height=150,
                                disabled=True
                            )

                    # 4. æ·»åŠ åˆ°ä¼šè¯å†å²
                    st.session_state.messages.append(AIMessage(content=answer))

                except Exception as e:
                    st.error(f"RAG è¿è¡Œå‡ºé”™ï¼š{e}")
                    import traceback
                    st.error(traceback.format_exc())
                    st.session_state.messages.append(AIMessage(content="æŠ±æ­‰ï¼Œç³»ç»Ÿåœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"))


if __name__ == "__main__":
    main()